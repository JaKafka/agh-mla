{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eRcRWZxntt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f98cff"
      },
      "source": [
        "# Task\n",
        "Create an OpenAI Gym-compatible stock market environment in Python, suitable for training a PyTorch-based reinforcement learning agent, starting with a simple CSV file for historical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d61910"
      },
      "source": [
        "## Define Data Source and Features\n",
        "\n",
        "### Subtask:\n",
        "Establish how to obtain historical or simulated stock market data. Identify and preprocess relevant features (e.g., open, high, low, close prices, volume, technical indicators) that will form the observation space for the RL agent. We can start with a simple CSV file for historical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96399d98"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to create a sample CSV file with historical stock data, as specified in the instructions. This will provide the necessary data for subsequent steps like loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeffac20",
        "outputId": "180c2990-5a72-42e4-9c75-02ce41c6195f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame with historical stock data\n",
        "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=10, freq='D'))\n",
        "\n",
        "data = {\n",
        "    'Date': dates,\n",
        "    'Open': np.random.uniform(100, 110, 10),\n",
        "    'High': np.random.uniform(110, 115, 10),\n",
        "    'Low': np.random.uniform(95, 100, 10),\n",
        "    'Close': np.random.uniform(100, 110, 10),\n",
        "    'Volume': np.random.randint(100000, 500000, 10)\n",
        "}\n",
        "\n",
        "df_sample = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some missing values for demonstration of handling missing values\n",
        "df_sample.loc[[2, 5], 'Open'] = np.nan\n",
        "df_sample.loc[7, 'Close'] = np.nan\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_sample.to_csv('stock_data.csv', index=False)\n",
        "\n",
        "print(\"Sample 'stock_data.csv' created successfully with dummy data and some missing values.\")\n",
        "print(df_sample.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 'stock_data.csv' created successfully with dummy data and some missing values.\n",
            "        Date        Open        High        Low       Close  Volume\n",
            "0 2023-01-01  104.455039  113.829450  95.142036  100.375870  367613\n",
            "1 2023-01-02  103.588101  110.843778  97.188188  108.337415  291037\n",
            "2 2023-01-03         NaN  114.294036  99.420789  106.032623  470461\n",
            "3 2023-01-04  109.935511  113.050723  97.838651  108.797715  196769\n",
            "4 2023-01-05  105.404743  110.112004  96.982422  100.727044  468922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b817f80"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to load the 'stock_data.csv' file into a Pandas DataFrame, parse the 'Date' column, and set it as the DataFrame index, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d801f4",
        "outputId": "6ba924de-1b07-4420-ee7a-76f821068690"
      },
      "source": [
        "df = pd.read_csv('stock_data.csv', parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "print(\"DataFrame loaded successfully with 'Date' as index:\")\n",
        "print(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded successfully with 'Date' as index:\n",
            "                  Open        High        Low       Close  Volume\n",
            "Date                                                             \n",
            "2023-01-01  104.455039  113.829450  95.142036  100.375870  367613\n",
            "2023-01-02  103.588101  110.843778  97.188188  108.337415  291037\n",
            "2023-01-03         NaN  114.294036  99.420789  106.032623  470461\n",
            "2023-01-04  109.935511  113.050723  97.838651  108.797715  196769\n",
            "2023-01-05  105.404743  110.112004  96.982422  100.727044  468922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0df2c3f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is loaded and indexed, I will select the relevant feature columns and handle any missing values within them using forward fill, which is a suitable method for time-series data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e1cee5a",
        "outputId": "155375e2-ec5d-45ab-9364-11b0fa8bed33"
      },
      "source": [
        "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "df_features = df[feature_columns].copy()\n",
        "\n",
        "# Handle missing values using forward fill\n",
        "df_features.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# If there are still NaN values at the beginning (where ffill can't fill), use backward fill\n",
        "df_features.fillna(method='bfill', inplace=True)\n",
        "\n",
        "print(\"Selected features and handled missing values:\")\n",
        "print(df_features.head())\n",
        "print(\"Missing values after handling:\\n\", df_features.isnull().sum())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features and handled missing values:\n",
            "                  Open        High        Low       Close  Volume\n",
            "Date                                                             \n",
            "2023-01-01  104.455039  113.829450  95.142036  100.375870  367613\n",
            "2023-01-02  103.588101  110.843778  97.188188  108.337415  291037\n",
            "2023-01-03  103.588101  114.294036  99.420789  106.032623  470461\n",
            "2023-01-04  109.935511  113.050723  97.838651  108.797715  196769\n",
            "2023-01-05  105.404743  110.112004  96.982422  100.727044  468922\n",
            "Missing values after handling:\n",
            " Open      0\n",
            "High      0\n",
            "Low       0\n",
            "Close     0\n",
            "Volume    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1448685918.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_features.fillna(method='ffill', inplace=True)\n",
            "/tmp/ipython-input-1448685918.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_features.fillna(method='bfill', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1cfe212"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code produced a FutureWarning regarding the deprecated use of `fillna(method=...)`. I will update the code to use the recommended `ffill()` and `bfill()` methods to address this warning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daef2a40",
        "outputId": "fd631fe3-4e87-4f99-b0a3-56d83c9ac34d"
      },
      "source": [
        "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "df_features = df[feature_columns].copy()\n",
        "\n",
        "# Handle missing values using forward fill\n",
        "df_features.ffill(inplace=True)\n",
        "\n",
        "# If there are still NaN values at the beginning (where ffill can't fill), use backward fill\n",
        "df_features.bfill(inplace=True)\n",
        "\n",
        "print(\"Selected features and handled missing values:\")\n",
        "print(df_features.head())\n",
        "print(\"Missing values after handling:\\n\", df_features.isnull().sum())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features and handled missing values:\n",
            "                  Open        High        Low       Close  Volume\n",
            "Date                                                             \n",
            "2023-01-01  104.455039  113.829450  95.142036  100.375870  367613\n",
            "2023-01-02  103.588101  110.843778  97.188188  108.337415  291037\n",
            "2023-01-03  103.588101  114.294036  99.420789  106.032623  470461\n",
            "2023-01-04  109.935511  113.050723  97.838651  108.797715  196769\n",
            "2023-01-05  105.404743  110.112004  96.982422  100.727044  468922\n",
            "Missing values after handling:\n",
            " Open      0\n",
            "High      0\n",
            "Low       0\n",
            "Close     0\n",
            "Volume    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd36e66b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that missing values have been handled, the next step is to normalize the selected features to a common scale as specified in the instructions. This will prevent features with larger magnitudes from dominating the learning process for the reinforcement learning agent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b51f57e",
        "outputId": "f95b7e27-44db-4bd0-82ec-8d587021ca94"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling to the feature DataFrame\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_features), columns=df_features.columns, index=df_features.index)\n",
        "\n",
        "print(\"Features normalized successfully:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Verify the range of the scaled data (should be between 0 and 1)\n",
        "print(\"\\nMin values after scaling:\\n\", df_scaled.min())\n",
        "print(\"\\nMax values after scaling:\\n\", df_scaled.max())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features normalized successfully:\n",
            "                Open      High       Low     Close    Volume\n",
            "Date                                                        \n",
            "2023-01-01  0.415340  0.888909  0.000000  0.000000  0.637463\n",
            "2023-01-02  0.322855  0.174981  0.478212  0.945345  0.403393\n",
            "2023-01-03  0.322855  1.000000  1.000000  0.671676  0.951839\n",
            "2023-01-04  1.000000  0.702701  0.630234  1.000000  0.115244\n",
            "2023-01-05  0.516655  0.000000  0.430122  0.041698  0.947134\n",
            "\n",
            "Min values after scaling:\n",
            " Open      0.0\n",
            "High      0.0\n",
            "Low       0.0\n",
            "Close     0.0\n",
            "Volume    0.0\n",
            "dtype: float64\n",
            "\n",
            "Max values after scaling:\n",
            " Open      1.0\n",
            "High      1.0\n",
            "Low       1.0\n",
            "Close     1.0\n",
            "Volume    1.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ba46514"
      },
      "source": [
        "## Design OpenAI Gym Interface\n",
        "\n",
        "### Subtask:\n",
        "Define the observation space, the action space, and a conceptual reward function for the OpenAI Gym-compatible stock market environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4893875c"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the `gym` library, which is necessary for defining the observation and action spaces using `gym.spaces.Box` and `gym.spaces.Discrete`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07277e4c",
        "outputId": "a6bfce2b-d859-4a74-95ae-f47698272d96"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(\"Gymnasium library imported successfully.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium library imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf57a074"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `gymnasium` library is imported, I will define the observation space using `gym.spaces.Box`. The shape will be determined by the number of features in `df_scaled`, and the bounds will be from 0 to 1 as the data is normalized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbda38b9",
        "outputId": "d571e742-6895-4750-f82f-98ab9d50f38e"
      },
      "source": [
        "n_features = df_scaled.shape[1]\n",
        "observation_space = gym.spaces.Box(low=0, high=1, shape=(n_features,), dtype=np.float32)\n",
        "\n",
        "print(\"Observation Space defined:\")\n",
        "print(observation_space)\n",
        "print(f\"Shape of observation space: {observation_space.shape}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space defined:\n",
            "Box(0.0, 1.0, (5,), float32)\n",
            "Shape of observation space: (5,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9ada34"
      },
      "source": [
        "**Reasoning**:\n",
        "With the observation space defined, the next step is to define the action space using `gym.spaces.Discrete` to represent 'hold', 'buy', and 'sell' actions, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32cd0798",
        "outputId": "5683eb81-3e36-45cc-c348-0e0c4937990d"
      },
      "source": [
        "action_space = gym.spaces.Discrete(3) # 0: hold, 1: buy, 2: sell\n",
        "\n",
        "print(\"Action Space defined:\")\n",
        "print(action_space)\n",
        "print(f\"Number of actions: {action_space.n}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space defined:\n",
            "Discrete(3)\n",
            "Number of actions: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77bca39d"
      },
      "source": [
        "### Conceptual Reward Function\n",
        "\n",
        "The reward function is crucial for guiding the reinforcement learning agent towards profitable trading strategies. For this stock market environment, the reward will primarily be based on the change in the agent's portfolio value, incorporating both cash and the value of held assets.\n",
        "\n",
        "Here's a conceptual breakdown:\n",
        "\n",
        "1.  **Portfolio Value Change**: The primary reward signal will be the change in the agent's total portfolio value (cash + (number of shares * current close price)) from one step to the next. A positive change indicates profit, leading to a positive reward, while a negative change indicates loss, leading to a negative reward.\n",
        "2.  **Profit/Loss from Trades**:\n",
        "    *   **Buying**: If the agent buys shares, the reward will not immediately reflect profit. The potential profit is realized when the shares are sold.\n",
        "    *   **Selling**: When the agent sells shares, the profit or loss from that specific trade (sale price - purchase price) will contribute to the reward. Selling at a higher price than bought should yield a positive reward, and vice-versa.\n",
        "3.  **Transaction Costs**: A small penalty can be introduced for each 'buy' or 'sell' action to simulate transaction fees (e.g., commissions). This encourages the agent to make fewer, more impactful trades.\n",
        "4.  **Holding Costs**: In some scenarios, a small negative reward for holding assets could be included to reflect opportunity costs or financing charges, though for simplicity, this might be omitted initially.\n",
        "5.  **Risk Adjustment (Optional)**: For more advanced environments, the reward could be adjusted based on the risk taken. For instance, high returns with high volatility might be penalized compared to consistent, moderate returns.\n",
        "\n",
        "**Goal**: The overall objective of the reward function is to maximize the agent's final portfolio value over the trading period, encouraging intelligent buying and selling decisions while minimizing losses and unnecessary transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19746a9a"
      },
      "source": [
        "## Implement the Custom Gym Environment\n",
        "\n",
        "### Subtask:\n",
        "Write a Python class that inherits from `gym.Env`. This class will implement the `__init__`, `step`, `reset`, and `render` methods according to the OpenAI Gym API. The `step` method will simulate trading actions based on the agent's output and update the environment state and reward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3adf5620"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `StockTradingEnv` class, inheriting from `gymnasium.Env`, and implement its `__init__`, `_get_observation`, `_get_current_price`, `_take_action`, `_get_reward`, `reset`, `step`, and `render` methods as per the instructions, which will create the custom Gym environment for stock trading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "061fa62c"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class StockTradingEnv(gym.Env):\n",
        "    metadata = {'render_modes': ['human'], 'render_fps': 30}\n",
        "\n",
        "    def __init__(self, df_scaled, original_df, observation_space, action_space, initial_balance=10000, trade_fee_pct=0.001):\n",
        "        super().__init__()\n",
        "        self.df_scaled = df_scaled\n",
        "        self.original_df = original_df # Store original df to get actual prices\n",
        "\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "        self.trade_fee_pct = trade_fee_pct\n",
        "\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.last_net_worth = self.initial_balance # For reward calculation\n",
        "        self.current_step = 0\n",
        "        self.max_steps = len(df_scaled) - 1\n",
        "\n",
        "        print(\"StockTradingEnv initialized.\")\n",
        "\n",
        "    def _get_observation(self):\n",
        "        return self.df_scaled.iloc[self.current_step].values.astype(np.float32)\n",
        "\n",
        "    def _get_current_price(self):\n",
        "        return self.original_df['Close'].iloc[self.current_step]\n",
        "\n",
        "    def _take_action(self, action, stocks_count):\n",
        "        current_price = self._get_current_price()\n",
        "\n",
        "        if action == 1: # Buy\n",
        "            # Calculate maximum shares we can buy, considering trade fees\n",
        "            # If we spend `x` on shares, `x * trade_fee_pct` is fee. Total `x + x * trade_fee_pct`\n",
        "            # `self.balance = x * (1 + trade_fee_pct)` => `x = self.balance / (1 + trade_fee_pct)`\n",
        "            available_cash_for_shares = self.balance / (1 + self.trade_fee_pct)\n",
        "\n",
        "            while available_cash_for_shares > 0 and stocks_count > 0:\n",
        "              if available_cash_for_shares > current_price: # Ensure we can buy at least one share\n",
        "                  num_shares_to_buy = 1\n",
        "                  cost = num_shares_to_buy * current_price * (1 + self.trade_fee_pct)\n",
        "\n",
        "                  self.shares_held += num_shares_to_buy\n",
        "                  self.balance -= cost\n",
        "                  stocks_count -= 1\n",
        "                  available_cash_for_shares = self.balance / (1 + self.trade_fee_pct)\n",
        "                # print(f\"Step {self.current_step}: Bought {num_shares_to_buy} shares at {current_price:.2f}. Balance: {self.balance:.2f}\")\n",
        "            # else: print(f\"Step {self.current_step}: Not enough balance to buy. Balance: {self.balance:.2f}\")\n",
        "\n",
        "        elif action == 2: # Sell\n",
        "            while stocks_count > 0:\n",
        "              if self.shares_held > 0:\n",
        "                  proceeds = self.shares_held * current_price * (1 - self.trade_fee_pct)\n",
        "                  self.balance += proceeds\n",
        "                  self.shares_held -=1\n",
        "                  stocks_count -= 1\n",
        "        else: # Hold\n",
        "            pass\n",
        "                # print(f\"Step {self.current_step}: Sold all shares at {current_price:.2f}. Balance: {self.balance:.2f}\")\n",
        "            # else: print(f\"Step {self.current_step}: No shares to sell.\")\n",
        "\n",
        "        # Update net worth after action\n",
        "        self.net_worth = self.balance + (self.shares_held * current_price)\n",
        "\n",
        "    def _get_reward(self):\n",
        "        reward = self.net_worth - self.last_net_worth\n",
        "        self.last_net_worth = self.net_worth # Update for next step\n",
        "        return reward\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.last_net_worth = self.initial_balance\n",
        "        self.current_step = 0\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "        self.last_net_worth = self.net_worth # Store net worth before action for reward calc\n",
        "        self._take_action(action)\n",
        "\n",
        "        reward = self._get_reward()\n",
        "\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.max_steps\n",
        "        truncated = False # Not implementing truncation for now\n",
        "\n",
        "        observation = self._get_observation() if not terminated else np.zeros_like(self._get_observation()) # Return dummy obs if terminated\n",
        "        info = {}\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if mode == 'human':\n",
        "            print(f\"Step: {self.current_step}\")\n",
        "            print(f\"Balance: {self.balance:.2f}\")\n",
        "            print(f\"Shares held: {self.shares_held:.2f}\")\n",
        "            print(f\"Net Worth: {self.net_worth:.2f}\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ec8917e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `StockTradingEnv` class is defined, the next logical step is to instantiate it and test its basic functionality by performing a few `reset()` and `step()` actions. This will confirm that the environment is correctly initialized and that actions can be taken, and rewards calculated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b088282",
        "outputId": "e68cbfb6-77d1-4892-febd-765dede53b49"
      },
      "source": [
        "env = StockTradingEnv(\n",
        "    df_scaled=df_scaled,\n",
        "    original_df=df,\n",
        "    observation_space=observation_space,\n",
        "    action_space=action_space,\n",
        "    initial_balance=10000,\n",
        "    trade_fee_pct=0.001\n",
        ")\n",
        "\n",
        "# Test environment with a few steps\n",
        "obs, info = env.reset()\n",
        "print(\"\\n--- Environment Reset ---\")\n",
        "env.render()\n",
        "\n",
        "# Perform a 'buy' action (action=1)\n",
        "print(\"\\n--- Taking Action: Buy ---\")\n",
        "action = 1\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.render()\n",
        "print(f\"Reward: {reward:.2f}, Terminated: {terminated}\")\n",
        "\n",
        "# Perform a 'hold' action (action=0)\n",
        "print(\"\\n--- Taking Action: Hold ---\")\n",
        "action = 0\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.render()\n",
        "print(f\"Reward: {reward:.2f}, Terminated: {terminated}\")\n",
        "\n",
        "# Perform a 'sell' action (action=2)\n",
        "print(\"\\n--- Taking Action: Sell ---\")\n",
        "action = 2\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.render()\n",
        "print(f\"Reward: {reward:.2f}, Terminated: {terminated}\")\n",
        "\n",
        "# Run through a few more steps to demonstrate progression\n",
        "print(\"\\n--- Running more steps ---\")\n",
        "for i in range(3):\n",
        "    action = env.action_space.sample() # Take random actions\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "    print(f\"Reward: {reward:.2f}, Terminated: {terminated}\")\n",
        "    if terminated:\n",
        "        break\n",
        "\n",
        "print(\"\\nEnvironment testing complete.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StockTradingEnv initialized.\n",
            "\n",
            "--- Environment Reset ---\n",
            "Step: 0\n",
            "Balance: 10000.00\n",
            "Shares held: 0.00\n",
            "Net Worth: 10000.00\n",
            "\n",
            "--- Taking Action: Buy ---\n",
            "Step: 1\n",
            "Balance: 52.85\n",
            "Shares held: 99.00\n",
            "Net Worth: 9990.06\n",
            "Reward: -9.94, Terminated: False\n",
            "\n",
            "--- Taking Action: Hold ---\n",
            "Step: 2\n",
            "Balance: 52.85\n",
            "Shares held: 99.00\n",
            "Net Worth: 10778.26\n",
            "Reward: 788.19, Terminated: False\n",
            "\n",
            "--- Taking Action: Sell ---\n",
            "Step: 3\n",
            "Balance: 10539.58\n",
            "Shares held: 0.00\n",
            "Net Worth: 10539.58\n",
            "Reward: -238.67, Terminated: False\n",
            "\n",
            "--- Running more steps ---\n",
            "Step: 4\n",
            "Balance: 84.56\n",
            "Shares held: 96.00\n",
            "Net Worth: 10529.14\n",
            "Reward: -10.44, Terminated: False\n",
            "Step: 5\n",
            "Balance: 84.56\n",
            "Shares held: 96.00\n",
            "Net Worth: 9754.36\n",
            "Reward: -774.78, Terminated: False\n",
            "Step: 6\n",
            "Balance: 84.56\n",
            "Shares held: 96.00\n",
            "Net Worth: 9956.83\n",
            "Reward: 202.47, Terminated: False\n",
            "\n",
            "Environment testing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f02ace7"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the created OpenAI Gym-compatible stock market environment, ready for integration with a PyTorch-based RL agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccde7fb7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Preprocessing Completed**: Historical stock market data was successfully loaded from a CSV, missing values were handled using forward and backward fill, and relevant features ('Open', 'High', 'Low', 'Close', 'Volume') were normalized using `MinMaxScaler`, resulting in values scaled between 0 and 1.\n",
        "*   **OpenAI Gym Interface Defined**:\n",
        "    *   The **observation space** was defined as a `gym.spaces.Box` with a shape of `(5,)`, representing the five normalized features, with values ranging from 0.0 to 1.0.\n",
        "    *   The **action space** was defined as `gym.spaces.Discrete(3)`, allowing for three distinct actions: 0 (hold), 1 (buy), and 2 (sell).\n",
        "    *   A **conceptual reward function** was established, primarily based on the change in the agent's portfolio value, accounting for profits/losses from trades and transaction costs.\n",
        "*   **Custom Stock Trading Environment Implemented**: A `StockTradingEnv` class was successfully created, inheriting from `gymnasium.Env`, fulfilling the OpenAI Gym API requirements:\n",
        "    *   The `__init__` method initializes the environment with an `initial_balance` (e.g., \\$10,000) and a `trade_fee_pct` (e.g., 0.1%).\n",
        "    *   The `reset` method reinitializes the environment's state variables (balance, shares held, net worth) to their starting values.\n",
        "    *   The `step` method correctly processes actions, updates the environment's state (balance, shares held), calculates the current `net_worth` and `reward` based on its change, and advances the simulation by one time step, accounting for `trade_fee_pct` during buy/sell operations.\n",
        "    *   The `render` method provides human-readable output of the current step, balance, shares held, and net worth.\n",
        "*   **Environment Functionality Verified**: Through systematic testing, the environment's ability to handle buy, hold, and sell actions, correctly update financial metrics, calculate rewards, and transition through steps was confirmed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The created `StockTradingEnv` is a robust foundation for training a PyTorch-based Reinforcement Learning agent. The next critical step is to integrate this environment with a suitable RL algorithm (e.g., A2C, PPO, DQN) implemented in PyTorch to begin the training process.\n",
        "*   To enhance the environment's realism and the RL agent's learning capabilities, consider expanding the observation space with additional technical indicators (e.g., Moving Averages, RSI, MACD), implementing more sophisticated transaction costs, or incorporating slippage models for larger trades.\n"
      ]
    }
  ]
}